"""
MDV Share Analyzer - Optimized Version
è£½å“ã‚·ã‚§ã‚¢è¦å› åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
Performance optimized with caching, modular structure, and better error handling
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor, RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm
import io
import json
import warnings
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from functools import lru_cache
import hashlib

warnings.filterwarnings('ignore')

# ========================
# Configuration Constants
# ========================
class Config:
    """Application configuration constants"""
    PAGE_TITLE = "è£½å“ã‚·ã‚§ã‚¢è¦å› åˆ†æãƒ„ãƒ¼ãƒ«"
    PAGE_ICON = "ğŸ“Š"
    LAYOUT = "wide"
    
    # Model parameters
    TEST_SIZE = 0.2
    RANDOM_STATE = 42
    CV_FOLDS = 5
    
    # Display parameters
    MAX_PREVIEW_ROWS = 10
    CHART_HEIGHT = 400
    CHART_WIDTH = 800
    
    # Cache settings
    CACHE_TTL = 3600  # 1 hour
    
    # Encoding options
    ENCODINGS = ["utf-8", "shift-jis", "cp932", "utf-8-sig"]
    DEFAULT_ENCODING_INDEX = 1

# ========================
# Data Classes
# ========================
@dataclass
class AnalysisResults:
    """Container for analysis results"""
    correlation_matrix: pd.DataFrame
    regression_results: Dict[str, Any]
    tree_results: Dict[str, Any]
    pca_results: Dict[str, Any]
    vif_results: Optional[pd.DataFrame] = None
    outliers: Optional[Dict[str, Any]] = None
    cross_validation_scores: Optional[Dict[str, float]] = None

# ========================
# Utility Functions
# ========================
@st.cache_data(ttl=Config.CACHE_TTL)
def load_and_preprocess_data(file_content: bytes, encoding: str) -> pd.DataFrame:
    """Load and preprocess CSV data with caching"""
    try:
        df = pd.read_csv(io.BytesIO(file_content), encoding=encoding)
        
        # Convert percentage strings to numeric
        for col in df.columns:
            if df[col].dtype == 'object':
                if df[col].astype(str).str.contains('%').any():
                    df[col] = (df[col].str.replace('%', '')
                              .replace('-', '0')
                              .astype(float))
        
        # Remove columns with all NaN values
        df = df.dropna(axis=1, how='all')
        
        # Fill NaN values with forward fill then backward fill
        df = df.fillna(method='ffill').fillna(method='bfill')
        
        return df
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return pd.DataFrame()

@st.cache_data
def calculate_vif(X: pd.DataFrame) -> pd.DataFrame:
    """Calculate Variance Inflation Factor for multicollinearity detection"""
    vif_data = pd.DataFrame()
    vif_data["Variable"] = X.columns
    vif_values = []
    
    for i in range(len(X.columns)):
        try:
            vif = variance_inflation_factor(X.values, i)
            vif_values.append(vif if vif < 1000 else np.inf)
        except:
            vif_values.append(np.nan)
    
    vif_data["VIF"] = vif_values
    vif_data["Multicollinearity"] = vif_data["VIF"].apply(
        lambda x: "High" if x > 10 else ("Moderate" if x > 5 else "Low")
    )
    return vif_data

@st.cache_data
def detect_outliers(data: pd.DataFrame, columns: List[str]) -> Dict[str, Any]:
    """Detect outliers using IQR and Z-score methods"""
    outliers = {}
    
    for col in columns:
        # IQR method
        Q1 = data[col].quantile(0.25)
        Q3 = data[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        # Z-score method
        z_scores = np.abs(stats.zscore(data[col].dropna()))
        
        outliers[col] = {
            'iqr_outliers': data[(data[col] < lower_bound) | (data[col] > upper_bound)].index.tolist(),
            'z_outliers': data.index[z_scores > 3].tolist() if len(z_scores) > 0 else [],
            'lower_bound': lower_bound,
            'upper_bound': upper_bound
        }
    
    return outliers

# ========================
# Analysis Functions
# ========================
@st.cache_data
def perform_correlation_analysis(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
    """Perform correlation analysis with caching"""
    return df[columns].corr()

@st.cache_data
def perform_regression_analysis(X: pd.DataFrame, y: pd.Series, 
                               model_type: str = "linear") -> Dict[str, Any]:
    """Perform regression analysis with multiple models"""
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=Config.TEST_SIZE, random_state=Config.RANDOM_STATE
    )
    
    # Select model
    models = {
        "linear": LinearRegression(),
        "ridge": Ridge(alpha=1.0),
        "lasso": Lasso(alpha=1.0),
        "elastic": ElasticNet(alpha=1.0)
    }
    
    model = models.get(model_type, LinearRegression())
    
    # Train model
    model.fit(X_train, y_train)
    
    # Predictions
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    # Metrics
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    test_mae = mean_absolute_error(y_test, y_pred_test)
    
    # Cross-validation
    cv_scores = cross_val_score(model, X, y, cv=Config.CV_FOLDS, 
                               scoring='r2')
    
    # Statsmodels for detailed statistics
    X_sm = sm.add_constant(X_train)
    model_sm = sm.OLS(y_train, X_sm).fit()
    
    # Feature importance
    if hasattr(model, 'coef_'):
        feature_importance = pd.DataFrame({
            'Variable': X.columns,
            'Coefficient': model.coef_,
            'Abs_Coefficient': np.abs(model.coef_)
        }).sort_values('Abs_Coefficient', ascending=False)
    else:
        feature_importance = None
    
    return {
        'model': model,
        'model_sm': model_sm,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'test_rmse': test_rmse,
        'test_mae': test_mae,
        'cv_scores': cv_scores,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'y_test': y_test,
        'y_pred': y_pred_test,
        'feature_importance': feature_importance,
        'X_train': X_train,
        'X_test': X_test,
        'y_train': y_train
    }

@st.cache_data
def perform_tree_analysis(X: pd.DataFrame, y: pd.Series, 
                         use_random_forest: bool = False) -> Dict[str, Any]:
    """Perform decision tree or random forest analysis"""
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=Config.TEST_SIZE, random_state=Config.RANDOM_STATE
    )
    
    # Select model
    if use_random_forest:
        model = RandomForestRegressor(
            n_estimators=100,
            max_depth=5,
            random_state=Config.RANDOM_STATE
        )
    else:
        model = DecisionTreeRegressor(
            max_depth=5,
            random_state=Config.RANDOM_STATE
        )
    
    # Train model
    model.fit(X_train, y_train)
    
    # Predictions
    y_pred = model.predict(X_test)
    
    # Metrics
    test_r2 = r2_score(y_test, y_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    
    # Feature importance
    importance_df = pd.DataFrame({
        'Variable': X.columns,
        'Importance': model.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    return {
        'model': model,
        'test_r2': test_r2,
        'test_rmse': test_rmse,
        'y_test': y_test,
        'y_pred': y_pred,
        'feature_importance': importance_df
    }

@st.cache_data
def perform_pca_analysis(X: pd.DataFrame, n_components: int = 2) -> Dict[str, Any]:
    """Perform PCA analysis"""
    # Standardize data
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # PCA
    pca = PCA(n_components=min(n_components, X.shape[1]))
    X_pca = pca.fit_transform(X_scaled)
    
    # Create component dataframe
    components_df = pd.DataFrame(
        X_pca,
        columns=[f'PC{i+1}' for i in range(X_pca.shape[1])]
    )
    
    # Feature loadings
    loadings = pd.DataFrame(
        pca.components_.T,
        columns=[f'PC{i+1}' for i in range(pca.n_components_)],
        index=X.columns
    )
    
    return {
        'pca': pca,
        'scaler': scaler,
        'X_pca': X_pca,
        'components_df': components_df,
        'loadings': loadings,
        'explained_variance_ratio': pca.explained_variance_ratio_,
        'cumulative_variance_ratio': np.cumsum(pca.explained_variance_ratio_)
    }

# ========================
# Visualization Functions
# ========================
def create_correlation_heatmap(corr_matrix: pd.DataFrame) -> go.Figure:
    """Create an interactive correlation heatmap"""
    fig = go.Figure(data=go.Heatmap(
        z=corr_matrix.values,
        x=corr_matrix.columns,
        y=corr_matrix.columns,
        colorscale='RdBu',
        zmid=0,
        text=corr_matrix.values.round(2),
        texttemplate='%{text}',
        textfont={"size": 10},
        colorbar=dict(title="ç›¸é–¢ä¿‚æ•°")
    ))
    
    fig.update_layout(
        title="ç›¸é–¢è¡Œåˆ—ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—",
        height=Config.CHART_HEIGHT,
        xaxis_title="",
        yaxis_title=""
    )
    
    return fig

def create_regression_plots(results: Dict[str, Any]) -> go.Figure:
    """Create regression analysis plots"""
    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=("äºˆæ¸¬å€¤ vs å®Ÿæ¸¬å€¤", "æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ")
    )
    
    # Prediction vs Actual
    fig.add_trace(
        go.Scatter(
            x=results['y_test'],
            y=results['y_pred'],
            mode='markers',
            name='äºˆæ¸¬å€¤',
            marker=dict(size=8, opacity=0.6)
        ),
        row=1, col=1
    )
    
    # Perfect prediction line
    min_val = min(results['y_test'].min(), results['y_pred'].min())
    max_val = max(results['y_test'].max(), results['y_pred'].max())
    fig.add_trace(
        go.Scatter(
            x=[min_val, max_val],
            y=[min_val, max_val],
            mode='lines',
            name='ç†æƒ³ç·š',
            line=dict(dash='dash', color='red')
        ),
        row=1, col=1
    )
    
    # Residuals
    residuals = results['y_test'] - results['y_pred']
    fig.add_trace(
        go.Scatter(
            x=results['y_pred'],
            y=residuals,
            mode='markers',
            name='æ®‹å·®',
            marker=dict(size=8, opacity=0.6)
        ),
        row=1, col=2
    )
    
    # Zero line for residuals
    fig.add_trace(
        go.Scatter(
            x=[results['y_pred'].min(), results['y_pred'].max()],
            y=[0, 0],
            mode='lines',
            name='ã‚¼ãƒ­ãƒ©ã‚¤ãƒ³',
            line=dict(dash='dash', color='red')
        ),
        row=1, col=2
    )
    
    fig.update_xaxes(title_text="å®Ÿæ¸¬å€¤", row=1, col=1)
    fig.update_yaxes(title_text="äºˆæ¸¬å€¤", row=1, col=1)
    fig.update_xaxes(title_text="äºˆæ¸¬å€¤", row=1, col=2)
    fig.update_yaxes(title_text="æ®‹å·®", row=1, col=2)
    
    fig.update_layout(
        height=Config.CHART_HEIGHT,
        showlegend=True,
        title_text=f"å›å¸°åˆ†æçµæœ (RÂ²={results['test_r2']:.4f})"
    )
    
    return fig

def create_feature_importance_chart(importance_df: pd.DataFrame, 
                                   chart_type: str = "bar") -> go.Figure:
    """Create feature importance visualization"""
    importance_df = importance_df.head(10)  # Top 10 features
    
    if chart_type == "bar":
        fig = px.bar(
            importance_df,
            x='Importance' if 'Importance' in importance_df.columns else 'Abs_Coefficient',
            y='Variable',
            orientation='h',
            title="ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆä¸Šä½10å¤‰æ•°ï¼‰"
        )
    else:
        fig = go.Figure(go.Pie(
            labels=importance_df['Variable'],
            values=importance_df['Importance'] if 'Importance' in importance_df.columns else importance_df['Abs_Coefficient'],
            hole=0.3
        ))
        fig.update_layout(title="ç‰¹å¾´é‡é‡è¦åº¦ã®å‰²åˆ")
    
    fig.update_layout(height=Config.CHART_HEIGHT)
    return fig

def create_pca_plots(pca_results: Dict[str, Any]) -> Tuple[go.Figure, go.Figure]:
    """Create PCA visualization plots"""
    # Scree plot
    fig_scree = go.Figure()
    fig_scree.add_trace(go.Bar(
        x=[f'PC{i+1}' for i in range(len(pca_results['explained_variance_ratio']))],
        y=pca_results['explained_variance_ratio'],
        name='å¯„ä¸ç‡'
    ))
    fig_scree.add_trace(go.Scatter(
        x=[f'PC{i+1}' for i in range(len(pca_results['cumulative_variance_ratio']))],
        y=pca_results['cumulative_variance_ratio'],
        mode='lines+markers',
        name='ç´¯ç©å¯„ä¸ç‡',
        yaxis='y2'
    ))
    
    fig_scree.update_layout(
        title="ä¸»æˆåˆ†ã®å¯„ä¸ç‡",
        yaxis=dict(title='å¯„ä¸ç‡'),
        yaxis2=dict(title='ç´¯ç©å¯„ä¸ç‡', overlaying='y', side='right'),
        height=Config.CHART_HEIGHT
    )
    
    # Biplot
    if pca_results['X_pca'].shape[1] >= 2:
        fig_biplot = go.Figure()
        
        # Scatter plot of samples
        fig_biplot.add_trace(go.Scatter(
            x=pca_results['X_pca'][:, 0],
            y=pca_results['X_pca'][:, 1],
            mode='markers',
            name='ã‚µãƒ³ãƒ—ãƒ«',
            marker=dict(size=8, opacity=0.6)
        ))
        
        # Loading vectors
        loadings = pca_results['loadings']
        for idx, var in enumerate(loadings.index):
            fig_biplot.add_annotation(
                x=loadings.iloc[idx, 0] * 3,
                y=loadings.iloc[idx, 1] * 3,
                ax=0, ay=0,
                xref="x", yref="y",
                axref="x", ayref="y",
                text=var,
                showarrow=True,
                arrowhead=2,
                arrowsize=1,
                arrowwidth=2,
                arrowcolor="red"
            )
        
        fig_biplot.update_layout(
            title="PCA Biplot",
            xaxis_title=f"PC1 ({pca_results['explained_variance_ratio'][0]:.1%})",
            yaxis_title=f"PC2 ({pca_results['explained_variance_ratio'][1]:.1%})",
            height=Config.CHART_HEIGHT
        )
    else:
        fig_biplot = go.Figure()
        fig_biplot.add_annotation(
            text="2ã¤ä»¥ä¸Šã®ä¸»æˆåˆ†ãŒå¿…è¦ã§ã™",
            xref="paper", yref="paper",
            x=0.5, y=0.5, showarrow=False
        )
    
    return fig_scree, fig_biplot

# ========================
# What-If Simulation
# ========================
def perform_what_if_simulation(model: Any, feature_names: List[str], 
                              base_values: Dict[str, float],
                              adjustments: Dict[str, float]) -> Dict[str, Any]:
    """Perform What-If simulation with sensitivity analysis"""
    # Create adjusted input
    adjusted_values = base_values.copy()
    for feature, adjustment in adjustments.items():
        if feature in adjusted_values:
            adjusted_values[feature] = adjustment
    
    # Convert to array for prediction
    X_base = np.array([base_values[f] for f in feature_names]).reshape(1, -1)
    X_adjusted = np.array([adjusted_values[f] for f in feature_names]).reshape(1, -1)
    
    # Predictions
    base_prediction = model.predict(X_base)[0]
    adjusted_prediction = model.predict(X_adjusted)[0]
    
    # Sensitivity analysis
    sensitivity = {}
    for feature in feature_names:
        X_temp = X_base.copy()
        X_temp[0, feature_names.index(feature)] *= 1.1  # 10% increase
        temp_prediction = model.predict(X_temp)[0]
        sensitivity[feature] = (temp_prediction - base_prediction) / base_prediction * 100
    
    return {
        'base_prediction': base_prediction,
        'adjusted_prediction': adjusted_prediction,
        'change': adjusted_prediction - base_prediction,
        'change_percent': (adjusted_prediction - base_prediction) / base_prediction * 100,
        'sensitivity': sensitivity,
        'adjusted_values': adjusted_values
    }

# ========================
# AI Prompt Generation
# ========================
def generate_analysis_prompt(analysis_results: Dict[str, Any], 
                           context: str = "",
                           level: str = "ä¸€èˆ¬") -> str:
    """Generate AI prompt for analysis interpretation"""
    # Format analysis results
    results_json = json.dumps(analysis_results, ensure_ascii=False, indent=2, default=str)
    
    # Level-specific instructions
    level_instructions = {
        "åˆå¿ƒè€…": "å°‚é–€ç”¨èªã‚’ä½¿ã‚ãšã€èª°ã«ã§ã‚‚åˆ†ã‹ã‚‹ã‚ˆã†ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
        "ä¸€èˆ¬": "ä¸€èˆ¬çš„ãªãƒ“ã‚¸ãƒã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³ãŒç†è§£ã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
        "å°‚é–€å®¶": "çµ±è¨ˆçš„ãªè©³ç´°ã‚‚å«ã‚ã¦ã€å°‚é–€çš„ã«è§£èª¬ã—ã¦ãã ã•ã„ã€‚"
    }
    
    prompt = f"""
ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœã‚’è§£é‡ˆã—ã¦ã€åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚

{level_instructions.get(level, level_instructions["ä¸€èˆ¬"])}

## åˆ†æã®èƒŒæ™¯
{context if context else "è£½å“ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚·ã‚§ã‚¢ã«å½±éŸ¿ã™ã‚‹è¦å› ã‚’åˆ†æã—ã¦ã„ã¾ã™ã€‚"}

## åˆ†æçµæœ
{results_json}

## è§£é‡ˆã—ã¦ã»ã—ã„ãƒã‚¤ãƒ³ãƒˆ

1. **ã“ã®åˆ†æçµæœã‹ã‚‰ä½•ãŒåˆ†ã‹ã‚‹ã‹**
   - æœ€ã‚‚é‡è¦ãªç™ºè¦‹ã¯ä½•ã‹
   - ã©ã®è¦å› ãŒæœ€ã‚‚å½±éŸ¿åŠ›ãŒã‚ã‚‹ã‹
   
2. **ãªãœãã®çµæœã«ãªã£ãŸã®ã‹**
   - çµ±è¨ˆçš„ã«è¦‹ã¦ä¿¡é ¼ã§ãã‚‹çµæœã‹
   - å¤‰æ•°é–“ã®é–¢ä¿‚æ€§ã‚’ã©ã†ç†è§£ã™ã¹ãã‹
   
3. **å®Ÿå‹™ã¸ã®æ´»ç”¨æ–¹æ³•**
   - ã“ã®çµæœã‚’ã©ã†æ´»ç”¨ã™ã‚Œã°ã‚ˆã„ã‹
   - å„ªå…ˆçš„ã«å–ã‚Šçµ„ã‚€ã¹ãã“ã¨ã¯ä½•ã‹
   
4. **æ³¨æ„ã™ã¹ãç‚¹**
   - ã“ã®åˆ†æã®é™ç•Œã¯ä½•ã‹
   - èª¤è§£ã—ã‚„ã™ã„ç‚¹ã¯ã‚ã‚‹ã‹

åˆ†æçµæœã‚’è¦‹ã¦ã€ä¸Šè¨˜ã®ãƒã‚¤ãƒ³ãƒˆã«ã¤ã„ã¦è§£èª¬ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
"""
    
    return prompt

# ========================
# Statistics Guide
# ========================
def show_statistics_guide():
    """Display statistics guide"""
    st.markdown("""
    <style>
    .stat-card {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
        margin-bottom: 20px;
    }
    .stat-title {
        color: #1f77b4;
        font-size: 20px;
        font-weight: bold;
        margin-bottom: 10px;
    }
    .formula {
        background-color: #ffffff;
        padding: 10px;
        border-radius: 5px;
        font-family: 'Courier New', monospace;
        margin: 10px 0;
    }
    </style>
    """, unsafe_allow_html=True)
    
    st.header("ğŸ“š çµ±è¨ˆã‚¬ã‚¤ãƒ‰")
    
    with st.expander("ğŸ“Š åŸºæœ¬çµ±è¨ˆé‡", expanded=False):
        st.markdown("""
        ### å¹³å‡ï¼ˆMeanï¼‰
        ãƒ‡ãƒ¼ã‚¿ã®ä¸­å¿ƒå‚¾å‘ã‚’è¡¨ã™æœ€ã‚‚åŸºæœ¬çš„ãªæŒ‡æ¨™
        
        ### ä¸­å¤®å€¤ï¼ˆMedianï¼‰
        ãƒ‡ãƒ¼ã‚¿ã‚’å¤§ãã•é †ã«ä¸¦ã¹ãŸã¨ãã®ä¸­å¤®ã®å€¤
        
        ### æ¨™æº–åå·®ï¼ˆStandard Deviationï¼‰
        ãƒ‡ãƒ¼ã‚¿ã®ã°ã‚‰ã¤ãã®å¤§ãã•ã‚’è¡¨ã™æŒ‡æ¨™
        
        ### å››åˆ†ä½æ•°ï¼ˆQuartilesï¼‰
        - Q1ï¼ˆç¬¬1å››åˆ†ä½æ•°ï¼‰ï¼šä¸‹ä½25%ã®ä½ç½®
        - Q2ï¼ˆç¬¬2å››åˆ†ä½æ•°ï¼‰ï¼šä¸­å¤®å€¤
        - Q3ï¼ˆç¬¬3å››åˆ†ä½æ•°ï¼‰ï¼šä¸Šä½25%ã®ä½ç½®
        """)
    
    with st.expander("ğŸ”— ç›¸é–¢åˆ†æ", expanded=False):
        st.markdown("""
        ### ç›¸é–¢ä¿‚æ•°ï¼ˆCorrelation Coefficientï¼‰
        - **ç¯„å›²**: -1 ï½ 1
        - **è§£é‡ˆ**:
          - 0.7ä»¥ä¸Š: å¼·ã„æ­£ã®ç›¸é–¢
          - 0.4ï½0.7: ä¸­ç¨‹åº¦ã®æ­£ã®ç›¸é–¢
          - -0.4ï½0.4: å¼±ã„ç›¸é–¢
          - -0.7ï½-0.4: ä¸­ç¨‹åº¦ã®è² ã®ç›¸é–¢
          - -0.7ä»¥ä¸‹: å¼·ã„è² ã®ç›¸é–¢
        
        ### æ³¨æ„ç‚¹
        - ç›¸é–¢é–¢ä¿‚ã¯å› æœé–¢ä¿‚ã‚’æ„å‘³ã—ãªã„
        - å¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’å—ã‘ã‚„ã™ã„
        - éç·šå½¢ãªé–¢ä¿‚ã¯æ¤œå‡ºã§ããªã„
        """)
    
    with st.expander("ğŸ“ˆ å›å¸°åˆ†æ", expanded=False):
        st.markdown("""
        ### RÂ²ã‚¹ã‚³ã‚¢ï¼ˆæ±ºå®šä¿‚æ•°ï¼‰
        - ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜åŠ›ã‚’è¡¨ã™æŒ‡æ¨™ï¼ˆ0ï½1ï¼‰
        - 0.7ä»¥ä¸Š: è‰¯å¥½ãªãƒ¢ãƒ‡ãƒ«
        - 0.5ï½0.7: ã¾ãšã¾ãšã®ãƒ¢ãƒ‡ãƒ«
        - 0.5æœªæº€: æ”¹å–„ãŒå¿…è¦
        
        ### RMSEï¼ˆäºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®ï¼‰
        äºˆæ¸¬èª¤å·®ã®å¤§ãã•ã‚’è¡¨ã™æŒ‡æ¨™ï¼ˆå°ã•ã„ã»ã©è‰¯ã„ï¼‰
        
        ### på€¤ï¼ˆP-valueï¼‰
        - 0.05æœªæº€: çµ±è¨ˆçš„ã«æœ‰æ„
        - 0.05ä»¥ä¸Š: çµ±è¨ˆçš„ã«æœ‰æ„ã§ãªã„
        
        ### å›å¸°ä¿‚æ•°
        å„èª¬æ˜å¤‰æ•°ãŒç›®çš„å¤‰æ•°ã«ä¸ãˆã‚‹å½±éŸ¿ã®å¤§ãã•
        """)
    
    with st.expander("ğŸŒ³ æ±ºå®šæœ¨åˆ†æ", expanded=False):
        st.markdown("""
        ### ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆFeature Importanceï¼‰
        å„å¤‰æ•°ã®äºˆæ¸¬ã¸ã®è²¢çŒ®åº¦ï¼ˆ0ï½1ã€åˆè¨ˆ1ï¼‰
        
        ### åˆ©ç‚¹
        - éç·šå½¢ãªé–¢ä¿‚ã‚’æ‰ãˆã‚‰ã‚Œã‚‹
        - è§£é‡ˆã—ã‚„ã™ã„
        - å¤–ã‚Œå€¤ã«å¼·ã„
        
        ### æ¬ ç‚¹
        - éå­¦ç¿’ã—ã‚„ã™ã„
        - ä¸å®‰å®šï¼ˆãƒ‡ãƒ¼ã‚¿ã®å°ã•ãªå¤‰åŒ–ã§çµæœãŒå¤§ããå¤‰ã‚ã‚‹ï¼‰
        """)
    
    with st.expander("ğŸ¯ ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰", expanded=False):
        st.markdown("""
        ### å¯„ä¸ç‡ï¼ˆExplained Variance Ratioï¼‰
        å„ä¸»æˆåˆ†ãŒèª¬æ˜ã™ã‚‹åˆ†æ•£ã®å‰²åˆ
        
        ### ç´¯ç©å¯„ä¸ç‡
        - 70%ä»¥ä¸Š: ååˆ†ãªæƒ…å ±ã‚’ä¿æŒ
        - 80%ä»¥ä¸Š: è‰¯å¥½
        - 90%ä»¥ä¸Š: éå¸¸ã«è‰¯å¥½
        
        ### ä¸»æˆåˆ†è² è·é‡ï¼ˆLoadingsï¼‰
        å„å¤‰æ•°ãŒä¸»æˆåˆ†ã«ä¸ãˆã‚‹å½±éŸ¿ã®å¤§ãã•
        """)
    
    with st.expander("âš ï¸ å¤šé‡å…±ç·šæ€§ï¼ˆVIFï¼‰", expanded=False):
        st.markdown("""
        ### VIFï¼ˆVariance Inflation Factorï¼‰
        - **1ï½5**: å¤šé‡å…±ç·šæ€§ãªã—
        - **5ï½10**: ä¸­ç¨‹åº¦ã®å¤šé‡å…±ç·šæ€§
        - **10ä»¥ä¸Š**: æ·±åˆ»ãªå¤šé‡å…±ç·šæ€§
        
        ### å¯¾å‡¦æ³•
        - ç›¸é–¢ã®é«˜ã„å¤‰æ•°ã®ä¸€æ–¹ã‚’é™¤å¤–
        - ä¸»æˆåˆ†åˆ†æã§æ¬¡å…ƒå‰Šæ¸›
        - Ridgeå›å¸°ã‚„Lassoå›å¸°ã‚’ä½¿ç”¨
        """)

# ========================
# Main Application
# ========================
def main():
    """Main application entry point"""
    # Page configuration
    st.set_page_config(
        page_title=Config.PAGE_TITLE,
        page_icon=Config.PAGE_ICON,
        layout=Config.LAYOUT
    )
    
    st.title(f"{Config.PAGE_ICON} {Config.PAGE_TITLE}")
    st.markdown("---")
    
    # Initialize session state
    if 'analysis_results' not in st.session_state:
        st.session_state.analysis_results = None
    if 'model' not in st.session_state:
        st.session_state.model = None
    if 'feature_names' not in st.session_state:
        st.session_state.feature_names = None
    
    # Sidebar
    with st.sidebar:
        st.header("ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
        
        uploaded_file = st.file_uploader(
            "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
            type=['csv'],
            help="åˆ†æã—ãŸã„CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
        )
        
        encoding = st.selectbox(
            "æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°",
            Config.ENCODINGS,
            index=Config.DEFAULT_ENCODING_INDEX,
            help="æ—¥æœ¬èªã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯'shift-jis'ã‚’é¸æŠã—ã¦ãã ã•ã„"
        )
        
        st.markdown("---")
        
        # Analysis settings
        if uploaded_file is not None:
            st.header("âš™ï¸ åˆ†æè¨­å®š")
            
            model_type = st.selectbox(
                "å›å¸°ãƒ¢ãƒ‡ãƒ«",
                ["linear", "ridge", "lasso", "elastic"],
                format_func=lambda x: {
                    "linear": "ç·šå½¢å›å¸°",
                    "ridge": "Ridgeå›å¸°",
                    "lasso": "Lassoå›å¸°",
                    "elastic": "Elastic Net"
                }[x]
            )
            
            use_random_forest = st.checkbox(
                "ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’ä½¿ç”¨",
                help="æ±ºå®šæœ¨ã®ä»£ã‚ã‚Šã«ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™"
            )
            
            perform_vif = st.checkbox(
                "VIFåˆ†æã‚’å®Ÿè¡Œ",
                value=True,
                help="å¤šé‡å…±ç·šæ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™"
            )
            
            detect_outliers_flag = st.checkbox(
                "å¤–ã‚Œå€¤æ¤œå‡ºã‚’å®Ÿè¡Œ",
                value=True,
                help="IQRã¨Z-scoreã§å¤–ã‚Œå€¤ã‚’æ¤œå‡ºã—ã¾ã™"
            )
        
        st.markdown("---")
        st.info(
            "ğŸ’¡ **ä½¿ã„æ–¹**\n"
            "1. CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n"
            "2. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’é¸æŠ\n"
            "3. èª¬æ˜å¤‰æ•°ã‚’é¸æŠ\n"
            "4. åˆ†æã‚’å®Ÿè¡Œ"
        )
    
    # Main content
    if uploaded_file is not None:
        try:
            # Load data with caching
            file_content = uploaded_file.read()
            uploaded_file.seek(0)  # Reset file pointer
            
            df = load_and_preprocess_data(file_content, encoding)
            
            if df.empty:
                st.error("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            
            # Data overview
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("è¡Œæ•°", f"{len(df):,}")
            with col2:
                st.metric("åˆ—æ•°", f"{len(df.columns):,}")
            with col3:
                st.metric("æ¬ æå€¤", f"{df.isnull().sum().sum():,}")
            with col4:
                st.metric("æ•°å€¤åˆ—", f"{len(df.select_dtypes(include=[np.number]).columns):,}")
            
            # Data preview
            with st.expander("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
                st.dataframe(df.head(Config.MAX_PREVIEW_ROWS))
                
                st.subheader("åŸºæœ¬çµ±è¨ˆé‡")
                st.dataframe(df.describe())
            
            st.markdown("---")
            
            # Variable selection
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) > 1:
                col1, col2 = st.columns(2)
                
                with col1:
                    target_col = st.selectbox(
                        "ğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆç›®çš„å¤‰æ•°ï¼‰",
                        numeric_cols,
                        help="äºˆæ¸¬ã—ãŸã„å¤‰æ•°ã‚’é¸æŠã—ã¦ãã ã•ã„"
                    )
                
                with col2:
                    feature_cols = st.multiselect(
                        "ğŸ“Š èª¬æ˜å¤‰æ•°",
                        [col for col in numeric_cols if col != target_col],
                        default=[col for col in numeric_cols if col != target_col][:5],
                        help="åˆ†æã«ä½¿ç”¨ã™ã‚‹å¤‰æ•°ã‚’é¸æŠã—ã¦ãã ã•ã„"
                    )
                
                if len(feature_cols) > 0:
                    # Analysis button
                    if st.button("ğŸš€ åˆ†æã‚’å®Ÿè¡Œ", type="primary", use_container_width=True):
                        with st.spinner("åˆ†æä¸­..."):
                            # Prepare data
                            X = df[feature_cols]
                            y = df[target_col]
                            
                            # Remove rows with NaN
                            mask = ~(X.isnull().any(axis=1) | y.isnull())
                            X = X[mask]
                            y = y[mask]
                            
                            # Initialize results container
                            analysis_summary = {
                                "ãƒ‡ãƒ¼ã‚¿æ¦‚è¦": {
                                    "ã‚µãƒ³ãƒ—ãƒ«æ•°": len(X),
                                    "èª¬æ˜å¤‰æ•°æ•°": len(feature_cols),
                                    "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°": target_col
                                }
                            }
                            
                            # Correlation analysis
                            corr_matrix = perform_correlation_analysis(df, feature_cols + [target_col])
                            
                            # Regression analysis
                            reg_results = perform_regression_analysis(X, y, model_type)
                            
                            # Store model in session state
                            st.session_state.model = reg_results['model']
                            st.session_state.feature_names = feature_cols
                            
                            # Tree analysis
                            tree_results = perform_tree_analysis(X, y, use_random_forest)
                            
                            # PCA analysis
                            pca_results = perform_pca_analysis(X)
                            
                            # Optional analyses
                            vif_results = None
                            if perform_vif:
                                vif_results = calculate_vif(X)
                            
                            outliers_results = None
                            if detect_outliers_flag:
                                outliers_results = detect_outliers(df, feature_cols)
                            
                            # Store results
                            st.session_state.analysis_results = AnalysisResults(
                                correlation_matrix=corr_matrix,
                                regression_results=reg_results,
                                tree_results=tree_results,
                                pca_results=pca_results,
                                vif_results=vif_results,
                                outliers=outliers_results
                            )
                            
                            # Update analysis summary
                            analysis_summary["å›å¸°åˆ†æ"] = {
                                "RÂ²ã‚¹ã‚³ã‚¢ï¼ˆãƒ†ã‚¹ãƒˆï¼‰": f"{reg_results['test_r2']:.4f}",
                                "RMSE": f"{reg_results['test_rmse']:.4f}",
                                "äº¤å·®æ¤œè¨¼RÂ²": f"{reg_results['cv_mean']:.4f} Â± {reg_results['cv_std']:.4f}"
                            }
                            
                            analysis_summary["æ±ºå®šæœ¨åˆ†æ"] = {
                                "RÂ²ã‚¹ã‚³ã‚¢": f"{tree_results['test_r2']:.4f}",
                                "RMSE": f"{tree_results['test_rmse']:.4f}"
                            }
                            
                            st.success("âœ… åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                    
                    # Display results if available
                    if st.session_state.analysis_results is not None:
                        results = st.session_state.analysis_results
                        
                        # Create tabs
                        tabs = st.tabs([
                            "ğŸ“Š ç›¸é–¢åˆ†æ",
                            "ğŸ“ˆ å›å¸°åˆ†æ",
                            "ğŸŒ³ æ±ºå®šæœ¨åˆ†æ",
                            "ğŸ¯ ä¸»æˆåˆ†åˆ†æ",
                            "âš ï¸ è¨ºæ–­",
                            "ğŸ“š çµ±è¨ˆã‚¬ã‚¤ãƒ‰",
                            "ğŸ”® What-If",
                            "ğŸ¤– AIé€£æº"
                        ])
                        
                        # Tab 1: Correlation Analysis
                        with tabs[0]:
                            st.header("ç›¸é–¢åˆ†æ")
                            fig_corr = create_correlation_heatmap(results.correlation_matrix)
                            st.plotly_chart(fig_corr, use_container_width=True)
                            
                            # Correlation with target
                            st.subheader(f"ã€Œ{target_col}ã€ã¨ã®ç›¸é–¢")
                            target_corr = results.correlation_matrix[target_col].sort_values(ascending=False)
                            st.dataframe(target_corr[target_corr.index != target_col])
                        
                        # Tab 2: Regression Analysis
                        with tabs[1]:
                            st.header("å›å¸°åˆ†æ")
                            
                            # Metrics
                            col1, col2, col3, col4 = st.columns(4)
                            with col1:
                                st.metric("RÂ² (ãƒ†ã‚¹ãƒˆ)", f"{results.regression_results['test_r2']:.4f}")
                            with col2:
                                st.metric("RMSE", f"{results.regression_results['test_rmse']:.4f}")
                            with col3:
                                st.metric("MAE", f"{results.regression_results['test_mae']:.4f}")
                            with col4:
                                st.metric("CV RÂ²", f"{results.regression_results['cv_mean']:.4f}")
                            
                            # Plots
                            fig_reg = create_regression_plots(results.regression_results)
                            st.plotly_chart(fig_reg, use_container_width=True)
                            
                            # Feature importance
                            if results.regression_results['feature_importance'] is not None:
                                st.subheader("å›å¸°ä¿‚æ•°")
                                fig_coef = create_feature_importance_chart(
                                    results.regression_results['feature_importance']
                                )
                                st.plotly_chart(fig_coef, use_container_width=True)
                            
                            # Statistical summary
                            if 'model_sm' in results.regression_results:
                                with st.expander("è©³ç´°ãªçµ±è¨ˆæƒ…å ±"):
                                    st.text(str(results.regression_results['model_sm'].summary()))
                        
                        # Tab 3: Decision Tree Analysis
                        with tabs[2]:
                            st.header("æ±ºå®šæœ¨åˆ†æ" + ("ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼‰" if use_random_forest else ""))
                            
                            # Metrics
                            col1, col2 = st.columns(2)
                            with col1:
                                st.metric("RÂ² (ãƒ†ã‚¹ãƒˆ)", f"{results.tree_results['test_r2']:.4f}")
                            with col2:
                                st.metric("RMSE", f"{results.tree_results['test_rmse']:.4f}")
                            
                            # Feature importance
                            st.subheader("ç‰¹å¾´é‡é‡è¦åº¦")
                            fig_tree = create_feature_importance_chart(
                                results.tree_results['feature_importance']
                            )
                            st.plotly_chart(fig_tree, use_container_width=True)
                            
                            # Importance table
                            st.dataframe(results.tree_results['feature_importance'])
                        
                        # Tab 4: PCA Analysis
                        with tabs[3]:
                            st.header("ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰")
                            
                            # Variance explained
                            fig_scree, fig_biplot = create_pca_plots(results.pca_results)
                            
                            col1, col2 = st.columns(2)
                            with col1:
                                st.plotly_chart(fig_scree, use_container_width=True)
                            with col2:
                                st.plotly_chart(fig_biplot, use_container_width=True)
                            
                            # Loadings
                            st.subheader("ä¸»æˆåˆ†è² è·é‡")
                            st.dataframe(results.pca_results['loadings'])
                        
                        # Tab 5: Diagnostics
                        with tabs[4]:
                            st.header("è¨ºæ–­")
                            
                            # VIF Analysis
                            if results.vif_results is not None:
                                st.subheader("å¤šé‡å…±ç·šæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆVIFï¼‰")
                                st.dataframe(results.vif_results)
                                
                                # Warning for high VIF
                                high_vif = results.vif_results[results.vif_results['VIF'] > 10]
                                if not high_vif.empty:
                                    st.warning(f"âš ï¸ ä»¥ä¸‹ã®å¤‰æ•°ã§å¤šé‡å…±ç·šæ€§ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {', '.join(high_vif['Variable'].tolist())}")
                            
                            # Outlier Detection
                            if results.outliers is not None:
                                st.subheader("å¤–ã‚Œå€¤æ¤œå‡º")
                                
                                outlier_summary = []
                                for col, outlier_info in results.outliers.items():
                                    iqr_count = len(outlier_info['iqr_outliers'])
                                    z_count = len(outlier_info['z_outliers'])
                                    outlier_summary.append({
                                        'å¤‰æ•°': col,
                                        'IQRå¤–ã‚Œå€¤': iqr_count,
                                        'Z-scoreå¤–ã‚Œå€¤': z_count,
                                        'ä¸‹é™': f"{outlier_info['lower_bound']:.2f}",
                                        'ä¸Šé™': f"{outlier_info['upper_bound']:.2f}"
                                    })
                                
                                st.dataframe(pd.DataFrame(outlier_summary))
                        
                        # Tab 6: Statistics Guide
                        with tabs[5]:
                            show_statistics_guide()
                        
                        # Tab 7: What-If Simulation
                        with tabs[6]:
                            st.header("ğŸ”® What-Ifã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
                            
                            if st.session_state.model is not None:
                                st.info("å¤‰æ•°ã®å€¤ã‚’èª¿æ•´ã—ã¦ã€äºˆæ¸¬å€¤ã¸ã®å½±éŸ¿ã‚’ç¢ºèªã§ãã¾ã™")
                                
                                # Get base values
                                base_values = {col: df[col].mean() for col in feature_cols}
                                adjusted_values = base_values.copy()
                                
                                # Variable adjustments
                                st.subheader("å¤‰æ•°ã®èª¿æ•´")
                                
                                # Create columns for sliders
                                n_cols = 2
                                cols = st.columns(n_cols)
                                
                                for i, var in enumerate(feature_cols):
                                    with cols[i % n_cols]:
                                        var_min = df[var].min()
                                        var_max = df[var].max()
                                        var_mean = df[var].mean()
                                        
                                        st.write(f"**{var}**")
                                        adjusted_values[var] = st.slider(
                                            f"",
                                            min_value=float(var_min),
                                            max_value=float(var_max),
                                            value=float(var_mean),
                                            step=float((var_max - var_min) / 100),
                                            key=f"whatif_slider_{var}"
                                        )
                                        
                                        # Show change
                                        change_pct = (adjusted_values[var] - base_values[var]) / base_values[var] * 100
                                        if abs(change_pct) > 0.1:
                                            st.caption(f"å¤‰åŒ–: {change_pct:+.1f}%")
                                
                                # Run simulation
                                if st.button("ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ", type="primary"):
                                    sim_results = perform_what_if_simulation(
                                        st.session_state.model,
                                        feature_cols,
                                        base_values,
                                        adjusted_values
                                    )
                                    
                                    # Display results
                                    st.markdown("---")
                                    st.subheader("ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœ")
                                    
                                    col1, col2, col3 = st.columns(3)
                                    with col1:
                                        st.metric(
                                            "ãƒ™ãƒ¼ã‚¹äºˆæ¸¬å€¤",
                                            f"{sim_results['base_prediction']:.4f}"
                                        )
                                    with col2:
                                        st.metric(
                                            "èª¿æ•´å¾Œäºˆæ¸¬å€¤",
                                            f"{sim_results['adjusted_prediction']:.4f}",
                                            f"{sim_results['change']:+.4f}"
                                        )
                                    with col3:
                                        st.metric(
                                            "å¤‰åŒ–ç‡",
                                            f"{sim_results['change_percent']:+.2f}%"
                                        )
                                    
                                    # Sensitivity analysis
                                    st.subheader("æ„Ÿåº¦åˆ†æï¼ˆ10%å¢—åŠ æ™‚ã®å½±éŸ¿ï¼‰")
                                    
                                    sensitivity_df = pd.DataFrame([
                                        {"å¤‰æ•°": k, "å½±éŸ¿åº¦(%)": v}
                                        for k, v in sim_results['sensitivity'].items()
                                    ]).sort_values("å½±éŸ¿åº¦(%)", key=abs, ascending=False)
                                    
                                    fig_sensitivity = px.bar(
                                        sensitivity_df,
                                        x="å½±éŸ¿åº¦(%)",
                                        y="å¤‰æ•°",
                                        orientation='h',
                                        title="å„å¤‰æ•°ã®æ„Ÿåº¦ï¼ˆ10%å¢—åŠ æ™‚ã®äºˆæ¸¬å€¤å¤‰åŒ–ç‡ï¼‰"
                                    )
                                    st.plotly_chart(fig_sensitivity, use_container_width=True)
                            else:
                                st.warning("å…ˆã«åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
                        
                        # Tab 8: AI Integration
                        with tabs[7]:
                            st.header("ğŸ¤– AIé€£æºï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆï¼‰")
                            
                            if st.session_state.analysis_results is not None:
                                st.info("åˆ†æçµæœã‚’è§£é‡ˆã™ã‚‹ãŸã‚ã®AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¾ã™")
                                
                                # Settings
                                col1, col2 = st.columns(2)
                                
                                with col1:
                                    interpretation_level = st.selectbox(
                                        "è§£é‡ˆãƒ¬ãƒ™ãƒ«",
                                        ["åˆå¿ƒè€…", "ä¸€èˆ¬", "å°‚é–€å®¶"],
                                        index=1,
                                        help="èª¬æ˜ã®è©³ç´°åº¦ã‚’é¸æŠã—ã¦ãã ã•ã„"
                                    )
                                
                                with col2:
                                    business_context = st.text_input(
                                        "ãƒ“ã‚¸ãƒã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä»»æ„ï¼‰",
                                        placeholder="ä¾‹ï¼šåŒ»è–¬å“ã®å¸‚å ´ã‚·ã‚§ã‚¢åˆ†æ",
                                        help="å…·ä½“çš„ãªæ¥­ç•Œã‚„çŠ¶æ³ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ã‚ˆã‚Šé©åˆ‡ãªè§£é‡ˆãŒå¾—ã‚‰ã‚Œã¾ã™"
                                    )
                                
                                # Generate prompt button
                                if st.button("ğŸ“ è§£é‡ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ", type="primary"):
                                    # Prepare analysis summary
                                    analysis_summary = {
                                        "ãƒ‡ãƒ¼ã‚¿æ¦‚è¦": {
                                            "ã‚µãƒ³ãƒ—ãƒ«æ•°": len(X),
                                            "èª¬æ˜å¤‰æ•°": feature_cols,
                                            "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°": target_col
                                        },
                                        "ç›¸é–¢åˆ†æ": {
                                            "æœ€å¤§ç›¸é–¢": float(results.correlation_matrix[target_col].abs().sort_values(ascending=False).iloc[1]),
                                            "ä¸Šä½ç›¸é–¢å¤‰æ•°": results.correlation_matrix[target_col].abs().sort_values(ascending=False).index[1:4].tolist()
                                        },
                                        "å›å¸°åˆ†æ": {
                                            "RÂ²ã‚¹ã‚³ã‚¢": results.regression_results['test_r2'],
                                            "RMSE": results.regression_results['test_rmse'],
                                            "äº¤å·®æ¤œè¨¼RÂ²": results.regression_results['cv_mean']
                                        }
                                    }
                                    
                                    if results.tree_results:
                                        analysis_summary["æ±ºå®šæœ¨åˆ†æ"] = {
                                            "RÂ²ã‚¹ã‚³ã‚¢": results.tree_results['test_r2'],
                                            "é‡è¦åº¦ä¸Šä½": results.tree_results['feature_importance'].head(3)['Variable'].tolist()
                                        }
                                    
                                    # Generate prompt
                                    prompt = generate_analysis_prompt(
                                        analysis_summary,
                                        business_context,
                                        interpretation_level
                                    )
                                    
                                    # Display prompt
                                    st.subheader("ğŸ“„ ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
                                    
                                    st.text_area(
                                        "ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚³ãƒ”ãƒ¼ã—ã¦AIã«è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„",
                                        value=prompt,
                                        height=400,
                                        key="generated_prompt_optimized"
                                    )
                                    
                                    # Copy instructions
                                    with st.expander("ğŸ’¡ ä½¿ã„æ–¹"):
                                        st.markdown("""
                                        ### ChatGPT
                                        1. https://chat.openai.com ã«ã‚¢ã‚¯ã‚»ã‚¹
                                        2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è²¼ã‚Šä»˜ã‘ã¦é€ä¿¡
                                        
                                        ### Claude
                                        1. https://claude.ai ã«ã‚¢ã‚¯ã‚»ã‚¹
                                        2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è²¼ã‚Šä»˜ã‘ã¦é€ä¿¡
                                        
                                        ### Google Gemini
                                        1. https://gemini.google.com ã«ã‚¢ã‚¯ã‚»ã‚¹
                                        2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è²¼ã‚Šä»˜ã‘ã¦é€ä¿¡
                                        """)
                                    
                                    # Download button
                                    st.download_button(
                                        label="ğŸ’¾ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                        data=prompt,
                                        file_name="analysis_prompt.txt",
                                        mime="text/plain"
                                    )
                            else:
                                st.warning("å…ˆã«åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
                
                else:
                    st.warning("èª¬æ˜å¤‰æ•°ã‚’é¸æŠã—ã¦ãã ã•ã„")
            else:
                st.error("æ•°å€¤åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚CSVãƒ•ã‚¡ã‚¤ãƒ«ã«æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            st.exception(e)
    
    else:
        # Welcome message
        st.info("ğŸ‘ˆ å·¦å´ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦é–‹å§‹ã—ã¦ãã ã•ã„")
        
        # Sample data generation
        with st.expander("ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"):
            if st.button("ã‚µãƒ³ãƒ—ãƒ«CSVã‚’ç”Ÿæˆ"):
                sample_data = pd.DataFrame({
                    'å¹´æœˆ': pd.date_range('2024-01', periods=12, freq='ME').strftime('%Y-%m'),
                    'ã‚·ã‚§ã‚¢': np.random.uniform(0.1, 0.5, 12),
                    'å£²ä¸Š': np.random.uniform(100000, 500000, 12),
                    'åºƒå‘Šè²»': np.random.uniform(10000, 50000, 12),
                    'åº—èˆ—æ•°': np.random.randint(10, 50, 12),
                    'é¡§å®¢æº€è¶³åº¦': np.random.uniform(3.0, 5.0, 12)
                })
                
                csv = sample_data.to_csv(index=False, encoding='utf-8-sig')
                st.download_button(
                    label="ğŸ“¥ ã‚µãƒ³ãƒ—ãƒ«CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv,
                    file_name="sample_share_data.csv",
                    mime="text/csv"
                )

if __name__ == "__main__":
    main()